{"cells":[{"cell_type":"code","source":["# Install semantic-link-labs library quietly\n","%pip install semantic-link-labs --q\n","\n","# Import required libraries\n","from pyspark.sql.functions import lit, col, expr, lpad, date_format, dayofweek, year, quarter, month, date_add, datediff, concat, current_timestamp\n","from pyspark.sql.types import StructType, StructField, StringType, BooleanType, DecimalType\n","from concurrent.futures import ThreadPoolExecutor, as_completed\n","from delta.tables import DeltaTable\n","import sempy.fabric as fabric\n","import sempy_labs as sl\n","import pandas as pd\n","import datetime\n","import time\n","import re"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":8,"statement_ids":[3,4,5,6,7,8],"state":"finished","livy_statement_state":"available","session_id":"23bfe1eb-0149-437c-9c35-6ab4324afbbf","normalized_state":"finished","queued_time":"2024-12-18T18:52:41.6851817Z","session_start_time":"2024-12-18T18:52:41.9870638Z","execution_start_time":"2024-12-18T18:52:54.7473467Z","execution_finish_time":"2024-12-18T18:53:20.4007003Z","parent_msg_id":"5b89eb05-143e-4664-9c88-4c77862530fd"},"text/plain":"StatementMeta(, 23bfe1eb-0149-437c-9c35-6ab4324afbbf, 8, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nfsspec-wrapper 0.1.14 requires PyJWT>=2.6.0, but you have pyjwt 2.4.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\nNote: you may need to restart the kernel to use updated packages.\nWarning: PySpark kernel has been restarted to use updated packages.\n\n"]}],"execution_count":1,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"3b066d0e-5164-4da5-824d-c1593c42a951"},{"cell_type":"code","source":["# Set functions for processing\n","\n","def get_lakehouse_from_catalog(\n","        lakehouse_name,\n","        create_if_not_exist = True,\n","        description = \"\",\n","        workspace_id = \"\"\n","    ):\n","        \"\"\"\n","        Checks if a Lakehouse exists in Microsoft Fabric and optionally creates it if it does not exist.\n","        \"\"\"\n","        try:\n","            # Try to get the Lakehouse using the mssparkutils.lakehouse.get() method\n","            mssparkutils.lakehouse.get(name=lakehouse_name, workspaceId=workspace_id)\n","            print(f\"Lakehouse '{lakehouse_name}' exists.\")\n","            return True\n","        except Exception as e:\n","            # If the Lakehouse does not exist, we can choose to create it\n","            if \"not found\" in str(e).lower():\n","                if create_if_not_exist:\n","                    print(f\"Creating Lakehouse '{lakehouse_name}'...\")\n","                    try:\n","                        mssparkutils.lakehouse.create(name=lakehouse_name, description=description, workspaceId=workspace_id)\n","                        print(f\"Lakehouse '{lakehouse_name}' created successfully.\")\n","                        return True\n","                    except Exception as create_error:\n","                        print(f\"Failed to create Lakehouse '{lakehouse_name}': {create_error}\")\n","                        raise Exception(f\"Failed to create Lakehouse '{lakehouse_name}': {create_error}\")\n","                return False\n","            else:\n","                # Re-raise any unexpected exceptions\n","                print(f\"Error occurred while retrieving Lakehouse '{lakehouse_name}': {e}\")\n","                raise Exception(f\"Error occurred while retrieving Lakehouse '{lakehouse_name}': {e}\")\n","\n","def poll_refresh_status(\n","        model, \n","        refresh_id, \n","        workspace, \n","        timeout=120, \n","        interval=5\n","    ):\n","    \"\"\"\n","    Polls the status of a refresh operation until completion or failure.\n","    Returns the final execution details.\n","    \"\"\"\n","    start_time = time.time()\n","    while True:\n","        # Check the status of the refresh operation\n","        details = fabric.get_refresh_execution_details(model, refresh_id, workspace)\n","        status = details.status\n","\n","        print(f'Current status: {status}')\n","\n","        # Exit loop if the operation has completed or failed\n","        if status in ['Completed', 'Failed']:\n","            print(f'Refresh operation finished with status: {status}')\n","            return details\n","\n","        # Check for timeout\n","        if time.time() - start_time > timeout:\n","            print('Timeout reached, exiting status check.')\n","            return details\n","\n","        # Wait briefly before the next status check\n","        time.sleep(interval)\n","\n","        # Define logging attributes\n","        log_attributes = [\n","            'start_time', 'end_time', 'type', 'commit_mode', \n","            'status', 'extended_status', 'current_refresh_type', 'number_of_attempts'\n","        ]\n","\n","        # Create a logging dictionary from execution details\n","        execution_log = {attr: getattr(execution_details, attr, None) for attr in log_attributes}\n","\n","        # Convert refresh details to a pandas DataFrame\n","        df_logging = pd.DataFrame([execution_log])\n","\n","        # Convert to a Spark DataFrame\n","        df_logging = spark.createDataFrame(df_logging)\n","\n","        # Add tagging columns to the DataFrame\n","        df_logging = (df_logging\n","                    .withColumn('capacity_metrics_workspace', lit(metrics_app_workspace))\n","                    .withColumn('capacity_metrics_model', lit(metrics_app_model)))\n","\n","        # Define the table name and directory path\n","        table = 'log_refresh_execution'\n","        directory = f'{lakehouse_file_path}/capacity_metrics/{table}'\n","        file_path = f\"{directory}/{datetime.datetime.now().strftime('%Y%m%d%H%M%S')}\"\n","\n","        # Write the DataFrame to a parquet file\n","        df_logging.write.format('parquet').save(file_path)\n","\n","        # Write the DataFrame to a Delta table\n","        df_logging.write.format('delta').mode('append').save(f'{lakehouse_table_path}/{table}')\n","\n","def clean_and_lowercase(name):\n","    \"\"\"\n","    Remove special characters from the name, replace them with underscores, trim leading and trailing underscores,\n","    and convert the result to lowercase.\n","    \"\"\"\n","    # Replace special characters with underscores\n","    name_with_underscores = re.sub(r'\\W+', '_', name)\n","\n","    # Trim leading/trailing underscores and convert to lowercase\n","    cleaned_name = re.sub(r'^_+|_+$', '', name_with_underscores).lower()\n","    \n","    return cleaned_name\n","\n","def parse_header(name):\n","    \"\"\"\n","    Extract text within brackets, remove special characters, then convert to lowercase.\n","    If no brackets are found, clean the name directly.\n","    \"\"\"\n","    # Search for text within brackets\n","    match = re.search(r'\\[(.*?)\\]', name)\n","    \n","    # Use found text or the whole name if no brackets are found\n","    return clean_and_lowercase(match.group(1)) if match else clean_and_lowercase(name)\n","\n","def evaluate_and_write_table(table_name):\n","    \"\"\"\n","    Evaluate a DAX query on the specified table, clean column names, convert the DataFrame to Spark format,\n","    and write it to parquet and delta formats.\n","    \"\"\"\n","    # Evaluate table using DAX and the Fabric Capacity model\n","    df_table = fabric.evaluate_dax(metrics_app_model, f\"EVALUATE '{table_name}'\", metrics_app_workspace)\n","    \n","    # Clean up column names\n","    df_table.columns = [parse_header(col) for col in df_table.columns]\n","    \n","    # Clean table name for use in paths\n","    clean_table_name = f'stage_{clean_and_lowercase(table_name)}'\n","    \n","    # Define file storage directory\n","    table_directory = f'{lakehouse_file_path}/capacity_metrics/{clean_table_name}'\n","    \n","    # File path with timestamp\n","    file_path = f\"{table_directory}/{datetime.datetime.now().strftime('%Y%m%d%H%M%S')}\"\n","\n","    # Convert DataFrame to Spark format\n","    df_table = spark.createDataFrame(df_table)\n","    \n","    # Save DataFrame in parquet format\n","    df_table.write.format('parquet').save(file_path)\n","    \n","    # Save DataFrame in delta format\n","    df_table.write.format('delta').mode('overwrite').save(f'{lakehouse_table_path}/{clean_table_name}')\n","\n","    # Create temp view\n","    df_table.createOrReplaceTempView(f'vw_{clean_table_name}')\n","\n","def process_capacity_app_table(table_name):\n","    \"\"\"\n","    Function used with threadpoolexecutor to capture data from semantic model tables in parallel.\n","    \"\"\"\n","    try:\n","        evaluate_and_write_table(table_name)\n","        return f'{table_name} completed successfully.'\n","    except Exception as exc:\n","        return f'Error processing {table_name}: {exc}'\n","\n","def upsert_to_table(config):\n","    \"\"\"\n","    Implements a generic upsert pattern for loading data into a dimensional layer in a Delta table.\n","    Maintains audit fields for data insertion and updates.\n","    \"\"\"\n","    table_name = config['table_name']\n","    lakehouse_table_path = config['lakehouse_table_path']\n","    df_source = config['source_dataframe']\n","    candidate_columns = config['candidate_columns']\n","    \n","    full_table_path = f'{lakehouse_table_path}/{table_name}'\n","    \n","    # Capture the current timestamp for audit fields\n","    current_ts = current_timestamp()\n","\n","    try:\n","        # Set update columns\n","        source_columns = df_source.columns\n","        \n","        # Add audit columns\n","        df_source = df_source \\\n","            .withColumn('insert_datetime', current_ts) \\\n","            .withColumn('update_datetime', current_ts)\n","        \n","        # Check if the Delta table exists\n","        if not DeltaTable.isDeltaTable(spark, full_table_path):\n","            print(f\"Table {table_name} does not exist. Creating it.\")\n","            \n","            df_source.write.format('delta').save(full_table_path)\n","            \n","            print(f\"{table_name} successfully created.\")\n","            return\n","\n","        # Set the Delta table reference\n","        target = DeltaTable.forPath(spark, full_table_path)\n","        \n","        # Dynamically set the merge condition using candidate_columns\n","        merge_condition = \" AND \".join([f\"target.`{col}` = source.`{col}`\" for col in candidate_columns])\n","\n","        # Dynamically set the update condition for columns where values differ\n","        update_condition = \" OR \".join([f\"target.`{col}` <> source.`{col}`\" for col in source_columns])\n","\n","        # Perform upsert operation\n","        merge_operation = target.alias('target').merge(\n","            source=df_source.alias('source'),\n","            condition=expr(merge_condition)\n","        ).whenMatchedUpdate(\n","            condition=expr(update_condition),\n","            set={\n","                col: f\"source.`{col}`\" for col in df_source.columns if col != 'insert_datetime'\n","            }\n","        ).whenNotMatchedInsertAll()\n","\n","        # Execute merge\n","        merge_operation.execute()\n","\n","        print(f\"{table_name} upsert completed successfully.\")\n","    except Exception as e:\n","        print(f'An error occurred in upsert_to_delta_table for table {table_name}: {e}')\n","        raise"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":94,"statement_ids":[94],"state":"finished","livy_statement_state":"available","session_id":"23bfe1eb-0149-437c-9c35-6ab4324afbbf","normalized_state":"finished","queued_time":"2024-12-18T19:44:18.0983042Z","session_start_time":null,"execution_start_time":"2024-12-18T19:44:18.4702367Z","execution_finish_time":"2024-12-18T19:44:18.7450615Z","parent_msg_id":"40db7d34-b3c0-40d1-b136-a2f08aa9c0c2"},"text/plain":"StatementMeta(, 23bfe1eb-0149-437c-9c35-6ab4324afbbf, 94, Finished, Available, Finished)"},"metadata":{}}],"execution_count":86,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"cd254eb4-fe4a-40ff-99f9-1cadca71ff0c"},{"cell_type":"code","source":["# Set monitoring lakehouse name\n","monitoring_lakehouse = 'lh_control'"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":87,"statement_ids":[87],"state":"finished","livy_statement_state":"available","session_id":"23bfe1eb-0149-437c-9c35-6ab4324afbbf","normalized_state":"finished","queued_time":"2024-12-18T19:38:26.009683Z","session_start_time":null,"execution_start_time":"2024-12-18T19:38:26.3898968Z","execution_finish_time":"2024-12-18T19:38:26.6209483Z","parent_msg_id":"09baf886-84b1-4c2f-b363-3e227eb6b4fd"},"text/plain":"StatementMeta(, 23bfe1eb-0149-437c-9c35-6ab4324afbbf, 87, Finished, Available, Finished)"},"metadata":{}}],"execution_count":79,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b4ce6371-1443-447e-aa33-f82e112e2a8e"},{"cell_type":"code","source":["# Check if lakehouse exists and create if not\n","get_lakehouse_from_catalog(\n","    lakehouse_name=monitoring_lakehouse\n",")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":88,"statement_ids":[88],"state":"finished","livy_statement_state":"available","session_id":"23bfe1eb-0149-437c-9c35-6ab4324afbbf","normalized_state":"finished","queued_time":"2024-12-18T19:38:26.7663061Z","session_start_time":null,"execution_start_time":"2024-12-18T19:38:27.0883511Z","execution_finish_time":"2024-12-18T19:38:27.9000447Z","parent_msg_id":"20775c83-1770-44b3-99ba-beb8e7cb99c7"},"text/plain":"StatementMeta(, 23bfe1eb-0149-437c-9c35-6ab4324afbbf, 88, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Lakehouse 'lh_control' exists.\n"]},{"output_type":"execute_result","execution_count":211,"data":{"text/plain":"True"},"metadata":{}}],"execution_count":80,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ddbe7d83-aede-4521-8389-48be2fa85e44"},{"cell_type":"code","source":["# Retrieve lakehouse details\n","df_lakehouse_details = mssparkutils.lakehouse.get(\n","    name=monitoring_lakehouse\n",")\n","\n","# Assign id values for lakehouse and workspace\n","lakehouse_id = df_lakehouse_details['id']\n","workspace_id = df_lakehouse_details['workspaceId']\n","\n","# Build lakehouse paths for writes\n","lakehouse_table_path = f'abfss://{workspace_id}@onelake.dfs.fabric.microsoft.com/{lakehouse_id}/Tables'\n","lakehouse_file_path = f'abfss://{workspace_id}@onelake.dfs.fabric.microsoft.com/{lakehouse_id}/Files'"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":89,"statement_ids":[89],"state":"finished","livy_statement_state":"available","session_id":"23bfe1eb-0149-437c-9c35-6ab4324afbbf","normalized_state":"finished","queued_time":"2024-12-18T19:38:27.668812Z","session_start_time":null,"execution_start_time":"2024-12-18T19:38:28.2634222Z","execution_finish_time":"2024-12-18T19:38:28.5149283Z","parent_msg_id":"340d9106-ec8e-475e-8bd6-9f08ddf63d26"},"text/plain":"StatementMeta(, 23bfe1eb-0149-437c-9c35-6ab4324afbbf, 89, Finished, Available, Finished)"},"metadata":{}}],"execution_count":81,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4ee6a97a-1706-4bf5-a2c2-8e8dd6118924"},{"cell_type":"markdown","source":["#### Update with location and name of your Fabric Capacity Metrics app\n","\n","```\n","# Define the location of the metrics application\n","metrics_app_model = 'Fabric Capacity Metrics'\n","metrics_app_workspace = 'Microsoft Fabric Capacity Metrics'\n","```"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"bdb18366-5ea5-4452-93f3-a29d9b7aece9"},{"cell_type":"code","source":["# Define the location of the metrics application\n","metrics_app_model = 'Fabric Capacity Metrics'\n","metrics_app_workspace = 'Fabric Capacity Metrics Monitoring'\n","\n","# Ensure model exists\n","try:\n","    sl.list_semantic_model_objects(\n","        dataset=metrics_app_model,\n","        workspace=metrics_app_workspace\n","    )\n","except Exception as e:\n","    print(f\"An error occurred while listing semantic model objects: {e}\")\n","    raise"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":90,"statement_ids":[90],"state":"finished","livy_statement_state":"available","session_id":"23bfe1eb-0149-437c-9c35-6ab4324afbbf","normalized_state":"finished","queued_time":"2024-12-18T19:38:29.4050256Z","session_start_time":null,"execution_start_time":"2024-12-18T19:38:29.7551106Z","execution_finish_time":"2024-12-18T19:38:32.1413773Z","parent_msg_id":"836df398-fc21-4294-9944-cbc29bd4ef14"},"text/plain":"StatementMeta(, 23bfe1eb-0149-437c-9c35-6ab4324afbbf, 90, Finished, Available, Finished)"},"metadata":{}}],"execution_count":82,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"2e5a4661-3057-427d-ad7c-6fe4f81bccd8"},{"cell_type":"code","source":["# Trigger refresh of the capacity metrics semantic model\n","refresh_id = fabric.refresh_dataset(\n","    dataset=metrics_app_model,\n","    workspace=metrics_app_workspace\n",")\n","\n","# Poll for refresh completion and capture logs\n","execution_details = poll_refresh_status(metrics_app_model, refresh_id, metrics_app_workspace)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":95,"statement_ids":[95],"state":"finished","livy_statement_state":"available","session_id":"23bfe1eb-0149-437c-9c35-6ab4324afbbf","normalized_state":"finished","queued_time":"2024-12-18T19:44:23.9641964Z","session_start_time":null,"execution_start_time":"2024-12-18T19:44:24.2729075Z","execution_finish_time":"2024-12-18T19:44:34.4563066Z","parent_msg_id":"8a5847a3-7904-43e2-a7f4-d5e18b5f00ba"},"text/plain":"StatementMeta(, 23bfe1eb-0149-437c-9c35-6ab4324afbbf, 95, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Current status: Unknown\nCurrent status: Completed\nRefresh operation finished with status: Completed\n"]}],"execution_count":87,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"484e2c2a-c70d-470e-ad4a-a5eab843dd1b"},{"cell_type":"code","source":["# List tables in the model and workspace\n","df_tables = fabric.list_tables(metrics_app_model, workspace=metrics_app_workspace)\n","\n","# Extract table names\n","table_names = df_tables['Name'].tolist()\n","\n","# Include only specific tables\n","include_tables = set([\n","    'MetricsByItemandOperationandHour',\n","    'StorageByWorkspacesandHour',\n","    'CUDetail',\n","    'MaxMemoryByItemAndHour',\n","    'Items'\n","])\n","\n","# Filter table names to include only specified tables\n","filtered_table_names = [table_name for table_name in table_names if table_name in include_tables]\n","\n","# Use ThreadPoolExecutor to process tables in parallel\n","results = []\n","with ThreadPoolExecutor() as executor:\n","    # Submit tasks to the executor\n","    future_to_table = {executor.submit(process_capacity_app_table, table_name): table_name for table_name in filtered_table_names}\n","    for future in as_completed(future_to_table):\n","        table_name = future_to_table[future]\n","        try:\n","            result = future.result()\n","            results.append(result)\n","        except Exception as exc:\n","            results.append(f'Error processing {table_name}: {exc}')\n","\n","# Print results\n","for result in results:\n","    print(result)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":92,"statement_ids":[92],"state":"finished","livy_statement_state":"available","session_id":"23bfe1eb-0149-437c-9c35-6ab4324afbbf","normalized_state":"finished","queued_time":"2024-12-18T19:38:32.6151538Z","session_start_time":null,"execution_start_time":"2024-12-18T19:38:44.9440949Z","execution_finish_time":"2024-12-18T19:39:15.8108896Z","parent_msg_id":"06893129-0dfa-4b19-941c-b0eebb818c79"},"text/plain":"StatementMeta(, 23bfe1eb-0149-437c-9c35-6ab4324afbbf, 92, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Items completed successfully.\nStorageByWorkspacesandHour completed successfully.\nMaxMemoryByItemAndHour completed successfully.\nCUDetail completed successfully.\nMetricsByItemandOperationandHour completed successfully.\n"]}],"execution_count":84,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false,"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a5827295-a4a5-444b-ac91-7c4ec0b4450c"},{"cell_type":"code","source":["# Define the schema for the workspace DataFrame\n","schema = StructType([\n","    StructField('id', StringType(), True),\n","    StructField('is_read_only', BooleanType(), True),\n","    StructField('is_on_dedicated_capacity', BooleanType(), True),\n","    StructField('capacity_id', StringType(), True),\n","    StructField('default_dataset_storage_format', StringType(), True),\n","    StructField('type', StringType(), True),\n","    StructField('name', StringType(), True),\n","])\n","\n","try:\n","    # Fetch workspace details from the fabric service\n","    df_stage = fabric.list_workspaces()\n","    \n","    # Convert the fetched data into a Spark DataFrame using the defined schema\n","    df_ws_stage_spark = spark.createDataFrame(df_stage, schema=schema)\n","    \n","    # Define the table name for storing workspace data\n","    table_name = 'stage_workspaces'\n","\n","    # Define file storage directory\n","    file_directory = f'{lakehouse_file_path}/capacity_metrics/{table_name}'\n","    \n","    # File path with timestamp\n","    file_path = f\"{file_directory}/{datetime.datetime.now().strftime('%Y%m%d%H%M%S')}\"\n","\n","    # Save DataFrame in parquet format\n","    df_ws_stage_spark.write.format('parquet').save(file_path)\n","\n","    # Write the DataFrame to a Delta table with an overwrite mode\n","    df_ws_stage_spark.write.format('delta').mode('overwrite').save(f'{lakehouse_table_path}/{table_name}')\n","    print(f'{table_name} staged successfully.')\n","\n","    # Create temp view\n","    df_ws_stage_spark.createOrReplaceTempView(f'vw_{table_name}')\n","\n","except Exception as e:\n","    print(f'An unexpected error occurred while capturing workspace details: {e}')\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":42,"statement_ids":[42],"state":"finished","livy_statement_state":"available","session_id":"23bfe1eb-0149-437c-9c35-6ab4324afbbf","normalized_state":"finished","queued_time":"2024-12-18T19:03:14.6520325Z","session_start_time":null,"execution_start_time":"2024-12-18T19:04:14.809807Z","execution_finish_time":"2024-12-18T19:04:18.3500216Z","parent_msg_id":"f78d501d-64e5-4468-9d99-94885bb488e9"},"text/plain":"StatementMeta(, 23bfe1eb-0149-437c-9c35-6ab4324afbbf, 42, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["stage_workspaces staged successfully.\n"]}],"execution_count":34,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false,"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ed0f134d-74e4-49a1-8744-3266c72fc18f"},{"cell_type":"code","source":["# Define the schema for the capacity DataFrame\n","schema = StructType([\n","    StructField('id', StringType(), True),\n","    StructField('display_name', StringType(), True),\n","    StructField('sku', StringType(), True),\n","    StructField('region', StringType(), True),\n","    StructField('state', StringType(), True),\n","])\n","\n","try:\n","    # Fetch capacity details from the fabric service\n","    df_stage = fabric.list_capacities()\n","    \n","    # Convert the raw data to a Spark DataFrame using the predefined schema\n","    df_stage_spark = spark.createDataFrame(df_stage, schema=schema)\n","    \n","    # Define the table name for storing capacity data\n","    table_name = 'stage_capacities'\n","\n","    # Define file storage directory\n","    file_directory = f'{lakehouse_file_path}/capacity_metrics/{table_name}'\n","    \n","    # File path with timestamp\n","    file_path = f\"{file_directory}/{datetime.datetime.now().strftime('%Y%m%d%H%M%S')}\"\n","\n","    # Save DataFrame in parquet format\n","    df_stage_spark.write.format('parquet').save(file_path)\n","\n","    # Write the DataFrame to a Delta table with an overwrite mode\n","    df_stage_spark.write.format('delta').mode('overwrite').save(f'{lakehouse_table_path}/{table_name}')\n","    print(f'{table_name} staged successfully.')\n","\n","    # Create temp view\n","    df_stage_spark.createOrReplaceTempView(f'vw_{table_name}')\n","\n","except Exception as e:\n","    print(f'An unexpected error occurred while capturing capacity details: {e}')\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":43,"statement_ids":[43],"state":"finished","livy_statement_state":"available","session_id":"23bfe1eb-0149-437c-9c35-6ab4324afbbf","normalized_state":"finished","queued_time":"2024-12-18T19:03:14.9251784Z","session_start_time":null,"execution_start_time":"2024-12-18T19:04:18.7030655Z","execution_finish_time":"2024-12-18T19:04:25.1031265Z","parent_msg_id":"15ff78ff-e6a7-4e3e-bf89-52f8b2264043"},"text/plain":"StatementMeta(, 23bfe1eb-0149-437c-9c35-6ab4324afbbf, 43, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["stage_capacities staged successfully.\n"]}],"execution_count":35,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e715b43b-d7a1-40b8-9aa3-64af0836c3de"},{"cell_type":"code","source":["# Define the schema for the item DataFrame\n","schema = StructType([\n","    StructField('id', StringType(), True),\n","    StructField('display_name', StringType(), True),\n","    StructField('description', StringType(), True),\n","    StructField('type', StringType(), True),\n","    StructField('workspace_id', StringType(), True),\n","])\n","\n","try:\n","    # Filter workspaces on dedicated capacity and collect their IDs\n","    df_workspaces = df_ws_stage_spark.filter(df_ws_stage_spark['is_on_dedicated_capacity'] == 1).select('id').collect()\n","    \n","    # Initialize an empty DataFrame to aggregate items from each workspace\n","    df_items_all = None\n","\n","    # Aggregate items from each workspace with dedicated capacity\n","    for ws in df_workspaces:\n","        ws_id = ws['id']\n","        df_items = spark.createDataFrame(fabric.list_items(workspace=ws_id), schema=schema)\n","        \n","        # Union the current items DataFrame with the aggregated items DataFrame\n","        if df_items_all is None:\n","            df_items_all = df_items\n","        else:\n","            df_items_all = df_items_all.union(df_items)\n","\n","    # Define the table name for storing item data\n","    table_name = 'stage_items_sl'\n","\n","    # Define file storage directory\n","    file_directory = f'{lakehouse_file_path}/capacity_metrics/{table_name}'\n","    \n","    # File path with timestamp\n","    file_path = f\"{file_directory}/{datetime.datetime.now().strftime('%Y%m%d%H%M%S')}\"\n","\n","    # Save DataFrame in parquet format\n","    df_items_all.write.format('parquet').save(file_path)\n","\n","    # Write the DataFrame to a Delta table with an overwrite mode\n","    df_items_all.write.format('delta').mode('overwrite').save(f'{lakehouse_table_path}/{table_name}')\n","    print(f'{table_name} staged successfully.')\n","\n","    # Create temp view\n","    df_items_all.createOrReplaceTempView(f'vw_{table_name}')\n","\n","except Exception as e:\n","    print(f'An unexpected error occurred while capturing item details: {e}')"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":44,"statement_ids":[44],"state":"finished","livy_statement_state":"available","session_id":"23bfe1eb-0149-437c-9c35-6ab4324afbbf","normalized_state":"finished","queued_time":"2024-12-18T19:03:14.935007Z","session_start_time":null,"execution_start_time":"2024-12-18T19:04:25.4529283Z","execution_finish_time":"2024-12-18T19:04:30.4011041Z","parent_msg_id":"2350d050-9430-4150-bf63-18fc6a8c2be1"},"text/plain":"StatementMeta(, 23bfe1eb-0149-437c-9c35-6ab4324afbbf, 44, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["stage_items_sl staged successfully.\n"]}],"execution_count":36,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false,"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"09ccbc87-505e-4d51-a8ae-26d4dac3114e"},{"cell_type":"code","source":["try:\n","    # Set table name\n","    table_name = 'stage_capacity_cost'\n","\n","    # Set github file path\n","    url_github = 'https://raw.githubusercontent.com/Lucid-Will/Lucid-Capacity-Monitoring/main/supporting_data_files/capacity_cost_by_region/capacity_cost_by_region.csv'\n","    \n","    # Load capacity cost file\n","    pd_stage = pd.read_csv(url_github)\n","    df_stage_spark = spark.createDataFrame(pd_stage)\n","    \n","    # Define file storage directory\n","    file_directory = f'{lakehouse_file_path}/capacity_metrics/{table_name}'\n","    \n","    # File path with timestamp\n","    file_path = f\"{file_directory}/{datetime.datetime.now().strftime('%Y%m%d%H%M%S')}\"\n","\n","    # Save DataFrame in parquet format\n","    df_stage_spark.write.format('parquet').save(file_path)\n","\n","    # Write the DataFrame to a Delta table with an overwrite mode\n","    df_stage_spark.write.format('delta').mode('overwrite').save(f'{lakehouse_table_path}/{table_name}')\n","    print(f'{table_name} staged successfully.')\n","\n","    # Create temp view\n","    df_stage_spark.createOrReplaceTempView(f'vw_{table_name}')\n","\n","except Exception as e:\n","    print(f'An unexpected error occurred while capturing capacity details: {e}')\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":67,"statement_ids":[67],"state":"finished","livy_statement_state":"available","session_id":"23bfe1eb-0149-437c-9c35-6ab4324afbbf","normalized_state":"finished","queued_time":"2024-12-18T19:20:46.2996035Z","session_start_time":null,"execution_start_time":"2024-12-18T19:20:46.72849Z","execution_finish_time":"2024-12-18T19:20:50.2703764Z","parent_msg_id":"ed1ef8e3-2475-4626-b12e-4c38c0e927a6"},"text/plain":"StatementMeta(, 23bfe1eb-0149-437c-9c35-6ab4324afbbf, 67, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["stage_capacity_cost staged successfully.\n"]}],"execution_count":59,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"22f7882e-3cdb-48f2-9400-4727a281a15b"},{"cell_type":"code","source":["try:\n","    # Set table name\n","    table_name = 'stage_storage_cost'\n","\n","    # Set GitHub file path\n","    url_github = 'https://raw.githubusercontent.com/Lucid-Will/Lucid-Capacity-Monitoring/main/supporting_data_files/onelake_storage_by_region/onelake_storage_by_region.csv'\n","    \n","    # Load storage cost file\n","    pd_stage = pd.read_csv(url_github)\n","\n","    # Create Spark DataFrame with the defined schema\n","    df_stage_spark = spark.createDataFrame(pd_stage)\n","    \n","    # Define file storage directory\n","    file_directory = f'{lakehouse_file_path}/capacity_metrics/{table_name}'\n","    \n","    # File path with timestamp\n","    file_path = f\"{file_directory}/{datetime.datetime.now().strftime('%Y%m%d%H%M%S')}\"\n","\n","    # Save DataFrame in parquet format\n","    df_stage_spark.write.format('parquet').save(file_path)\n","\n","    # Write the DataFrame to a Delta table with an overwrite mode\n","    df_stage_spark.write.format('delta').mode('overwrite').save(f'{lakehouse_table_path}/{table_name}')\n","    print(f'{table_name} staged successfully.')\n","\n","    # Create temp view\n","    df_stage_spark.createOrReplaceTempView(f'vw_{table_name}')\n","\n","except Exception as e:\n","    print(f'An unexpected error occurred while capturing storage details: {e}')\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":46,"statement_ids":[46],"state":"finished","livy_statement_state":"available","session_id":"23bfe1eb-0149-437c-9c35-6ab4324afbbf","normalized_state":"finished","queued_time":"2024-12-18T19:03:15.1477941Z","session_start_time":null,"execution_start_time":"2024-12-18T19:04:34.644733Z","execution_finish_time":"2024-12-18T19:04:38.1764966Z","parent_msg_id":"3007d100-0883-48e4-90f9-020c4f803fe5"},"text/plain":"StatementMeta(, 23bfe1eb-0149-437c-9c35-6ab4324afbbf, 46, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["stage_storage_cost staged successfully.\n"]}],"execution_count":38,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"76f44534-5d26-4522-beea-0407e1acf7f6"},{"cell_type":"code","source":["try:\n","    # Assign table name\n","    table_name = 'stage_date'\n","\n","    # Check if the date table already exists\n","    if DeltaTable.isDeltaTable(spark, f'{lakehouse_table_path}/{table_name}'):\n","        print(f'Table {table_name} already exists. Skipping creation.')\n","        df_date = spark.read.format(\"delta\").load(f'{lakehouse_table_path}/{table_name}')\n","    else:\n","        print(f'Table {table_name} does not exist. Creating it.')\n","\n","        # Define your date range\n","        start_date = '1901-01-01'\n","        end_date = '2100-12-31'\n","\n","        # Create a DataFrame with the date range difference in days\n","        date_diff = spark.createDataFrame([(start_date, end_date)], ['start', 'end'])\n","        date_diff = date_diff.withColumn('diff', datediff(col('end'), col('start'))).collect()[0]['diff']\n","\n","        # Generate a DataFrame with the sequence of dates\n","        date_range_diff = spark.range(0, date_diff + 1).withColumnRenamed('id', 'day_id')\n","        start_date_df = spark.createDataFrame([(start_date,)], ['start_date'])\n","\n","        # Cast 'day_id' to integer and create a sequence of dates by adding it to 'start_date'\n","        df_date = start_date_df.crossJoin(date_range_diff) \\\n","            .select(date_add(col('start_date'), col('day_id').cast('int')).alias('date'))\n","\n","        # Enrich the DataFrame with date attributes\n","        df_date = df_date.select(\n","            date_format(col('date'), 'yyyyMMdd').cast('int').alias('calendar_date_key'),\n","            col('date').alias('calendar_date'),\n","            dayofweek(col('date')).cast('int').alias('calendar_weekday_number'),\n","            date_format(col('date'), 'EEEE').alias('calendar_weekday'),\n","            lpad(month(col('date')).cast('string'), 2, '0').alias('calendar_month_number'),\n","            date_format(col('date'), 'MMMM').alias('calendar_month'),\n","            concat(year(col('date')), lpad(month(col('date')).cast('string'), 2, '0')).alias('calendar_year_month_number'),\n","            date_format(col('date'), 'MMMM yyyy').alias('calendar_month_year'),\n","            quarter(col('date')).cast('string').alias('calendar_quarter_number'),\n","            concat(lit('Q'), quarter(col('date')).cast('string')).alias('calendar_quarter'),\n","            year(col('date')).alias('calendar_year_number')\n","        ).distinct()\n","\n","        # Write to delta table\n","        df_date.write.format('delta').save(f'{lakehouse_table_path}/{table_name}')\n","        print(f'Table {table_name} created.')\n","\n","    # Create temp view\n","    df_date.createOrReplaceTempView(f'vw_{table_name}')\n","\n","except Exception as e:\n","    print(f'An unexpected error occurred while capturing date details: {e}')\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":47,"statement_ids":[47],"state":"finished","livy_statement_state":"available","session_id":"23bfe1eb-0149-437c-9c35-6ab4324afbbf","normalized_state":"finished","queued_time":"2024-12-18T19:03:15.1888303Z","session_start_time":null,"execution_start_time":"2024-12-18T19:04:38.5535721Z","execution_finish_time":"2024-12-18T19:04:38.7809199Z","parent_msg_id":"9fe8c036-2db0-436c-b9c0-e4717a9b6f8c"},"text/plain":"StatementMeta(, 23bfe1eb-0149-437c-9c35-6ab4324afbbf, 47, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Table stage_date already exists. Skipping creation.\n"]}],"execution_count":39,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f5487a74-c5a3-4e8f-9144-0ed25e918c1a"},{"cell_type":"code","source":["# Assign measure table name\n","table_name = 'measure_table'\n","\n","try:\n","    # Check if the measure table already exists\n","    if DeltaTable.isDeltaTable(spark, f'{lakehouse_table_path}/{table_name}'):\n","        print(f'Table {table_name} already exists. Skipping creation.')\n","    else:\n","        # Create measure table shell since it does not exist\n","        print(f'Table {table_name} does not exist. Creating it.')\n","\n","        df_measure = spark.sql(\"\"\"\n","            SELECT 1 AS Value\n","        \"\"\")\n","\n","        # Write the DataFrame to a delta table\n","        df_measure.write.format('delta').save(f'{lakehouse_table_path}/{table_name}')\n","\n","        print(f'Table {table_name} created.')\n","\n","except Exception as e:\n","    print(f'An unexpected error occurred while creating {table_name}: {e}')"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":48,"statement_ids":[48],"state":"finished","livy_statement_state":"available","session_id":"23bfe1eb-0149-437c-9c35-6ab4324afbbf","normalized_state":"finished","queued_time":"2024-12-18T19:03:15.283155Z","session_start_time":null,"execution_start_time":"2024-12-18T19:04:39.1118866Z","execution_finish_time":"2024-12-18T19:04:39.3384343Z","parent_msg_id":"090ec7c5-3533-4202-a741-1a614cc9a51e"},"text/plain":"StatementMeta(, 23bfe1eb-0149-437c-9c35-6ab4324afbbf, 48, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Table measure_table already exists. Skipping creation.\n"]}],"execution_count":40,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"3148ab0e-4ede-4518-b087-0797ba63535e"},{"cell_type":"code","source":["# Stage and load workspace\n","df_workspace = spark.sql(\"\"\"\n","    SELECT UPPER(id)    workspace_id,\n","           name         workspace_name,\n","           capacity_id\n","    FROM   vw_stage_workspaces\n","    WHERE  capacity_id IS NOT NULL\n","\"\"\")\n","\n","# Define configuration for the upsert operation\n","config = {\n","    'table_name': 'dim_workspace',\n","    'lakehouse_table_path': lakehouse_table_path,\n","    'source_dataframe': df_workspace,\n","    'candidate_columns': ['workspace_id']\n","}\n","\n","# Execute upsert to the dimensional table\n","upsert_to_table(config)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":49,"statement_ids":[49],"state":"finished","livy_statement_state":"available","session_id":"23bfe1eb-0149-437c-9c35-6ab4324afbbf","normalized_state":"finished","queued_time":"2024-12-18T19:03:15.3926797Z","session_start_time":null,"execution_start_time":"2024-12-18T19:04:39.7063086Z","execution_finish_time":"2024-12-18T19:04:41.2184543Z","parent_msg_id":"8821886b-2d70-4df5-a088-35df0166b66d"},"text/plain":"StatementMeta(, 23bfe1eb-0149-437c-9c35-6ab4324afbbf, 49, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Table dim_workspace does not exist. Creating it.\ndim_workspace successfully created.\n"]}],"execution_count":41,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false,"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"746e0667-ac25-420e-8e1e-f0da5fa0bd86"},{"cell_type":"code","source":["# Stage and load capacity\n","df_capacity = spark.sql(\"\"\"\n","    WITH storage AS (\n","        SELECT region, cost\n","        FROM   vw_stage_storage_cost\n","        WHERE  storage_type = 'OneLake storage'\n","    ),\n","    bcdr AS (\n","        SELECT region, cost\n","        FROM   vw_stage_storage_cost\n","        WHERE  storage_type = 'OneLake BCDR storage'\n","    ),\n","    cache AS (\n","        SELECT region, cost\n","        FROM   vw_stage_storage_cost\n","        WHERE  storage_type = 'OneLake cache'\n","    )\n","    SELECT DISTINCT \n","           UPPER(a.id)         capacity_id,\n","           a.display_name      capacity_name,\n","           a.sku,\n","           a.region,\n","           b.capacity_unit,\n","           b.pay_go_hour,\n","           b.reservation_hour,\n","           c.cost              onelake_storage_cost,\n","           d.cost              onelake_bcdr_storage_cost,\n","           e.cost              onelake_cache_cost,\n","           a.state\n","    FROM   vw_stage_capacities a\n","    LEFT JOIN vw_stage_capacity_cost b\n","        ON a.region = b.region\n","       AND a.sku = b.sku\n","    LEFT JOIN storage c\n","        ON a.region = c.region\n","    LEFT JOIN bcdr d\n","        ON a.region = d.region\n","    LEFT JOIN cache e\n","        ON a.region = e.region\n","    WHERE  capacity_unit IS NOT NULL\n","\"\"\")\n","\n","# Define configuration for upsert operation\n","config = {\n","    'table_name': 'dim_capacity',\n","    'lakehouse_table_path': lakehouse_table_path,\n","    'source_dataframe': df_capacity,\n","    'candidate_columns': ['capacity_id']\n","}\n","\n","# Execute upsert to the dimensional table\n","upsert_to_table(config)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":74,"statement_ids":[74],"state":"finished","livy_statement_state":"available","session_id":"23bfe1eb-0149-437c-9c35-6ab4324afbbf","normalized_state":"finished","queued_time":"2024-12-18T19:22:26.4426598Z","session_start_time":null,"execution_start_time":"2024-12-18T19:22:26.7860656Z","execution_finish_time":"2024-12-18T19:22:29.2369527Z","parent_msg_id":"66662cb7-975d-4392-9208-5cee7774168b"},"text/plain":"StatementMeta(, 23bfe1eb-0149-437c-9c35-6ab4324afbbf, 74, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["dim_capacity upsert completed successfully.\n"]}],"execution_count":66,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"8a98007a-d8ae-4915-9b86-a62a3f6f40cb"},{"cell_type":"code","source":["# Stage and load item\n","df_item = spark.sql(\"\"\"\n","\tSELECT UPPER(itemid)            item_id,\n","           UPPER(workspaceid)       workspace_id,\n","           itemname                 item_name,\n","           itemkind                 item_type\n","    FROM vw_stage_items\n","\"\"\")\n","\n","# Define configuration for the upsert operation\n","config = {\n","    'table_name': 'dim_item',\n","    'lakehouse_table_path': lakehouse_table_path,\n","    'source_dataframe': df_item,\n","    'candidate_columns': ['item_id', 'workspace_id', 'item_name']\n","}\n","\n","# Execute upsert to the dimensional table\n","upsert_to_table(config)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":75,"statement_ids":[75],"state":"finished","livy_statement_state":"available","session_id":"23bfe1eb-0149-437c-9c35-6ab4324afbbf","normalized_state":"finished","queued_time":"2024-12-18T19:22:34.5243415Z","session_start_time":null,"execution_start_time":"2024-12-18T19:22:34.9422247Z","execution_finish_time":"2024-12-18T19:22:38.4520413Z","parent_msg_id":"c57bb43d-a412-4b55-86c2-759f461ff1ea"},"text/plain":"StatementMeta(, 23bfe1eb-0149-437c-9c35-6ab4324afbbf, 75, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["dim_item upsert completed successfully.\n"]}],"execution_count":67,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"2132928c-d234-4fb2-a11c-07b7f23897fd"},{"cell_type":"code","source":["# Stage and load date\n","df_date = spark.sql(\"\"\"\n","    SELECT DISTINCT d.*\n","    FROM   vw_stage_date d\n","    INNER JOIN vw_stage_metricsbyitemandoperationandhour m \n","        ON d.calendar_date = m.date\n","\"\"\")\n","\n","# Define configuration for the upsert operation\n","config = {\n","    'table_name': 'dim_date',\n","    'lakehouse_table_path': lakehouse_table_path,\n","    'source_dataframe': df_date,\n","    'candidate_columns': ['calendar_date_key']\n","}\n","\n","# Execute upsert to the dimensional table\n","upsert_to_table(config)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":76,"statement_ids":[76],"state":"finished","livy_statement_state":"available","session_id":"23bfe1eb-0149-437c-9c35-6ab4324afbbf","normalized_state":"finished","queued_time":"2024-12-18T19:22:34.6210266Z","session_start_time":null,"execution_start_time":"2024-12-18T19:22:38.793645Z","execution_finish_time":"2024-12-18T19:22:43.6536695Z","parent_msg_id":"e628bf2f-9310-41c2-9c8f-cabeb316bc0d"},"text/plain":"StatementMeta(, 23bfe1eb-0149-437c-9c35-6ab4324afbbf, 76, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["dim_date upsert completed successfully.\n"]}],"execution_count":68,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"3fbeb71c-988e-4590-91fe-a9e6d63c6328"},{"cell_type":"code","source":["# Stage and load operation\n","df_operation = spark.sql(\"\"\"\n","    SELECT DISTINCT\n","           abs(hash(operationname)) operation_id,\n","           operationname            operation_name\n","    FROM vw_stage_metricsbyitemandoperationandhour\n","    UNION\n","    SELECT DISTINCT\n","           abs(hash(operationname)) operation_id,\n","           operationname            operation_name\n","    FROM vw_stage_storagebyworkspacesandhour\n","\"\"\")\n","\n","# Define configuration for the upsert operation\n","config = {\n","    'table_name': 'dim_operation',\n","    'lakehouse_table_path': lakehouse_table_path,\n","    'source_dataframe': df_operation,\n","    'candidate_columns': ['operation_id']\n","}\n","\n","# Execute upsert to the dimensional table\n","upsert_to_table(config)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":77,"statement_ids":[77],"state":"finished","livy_statement_state":"available","session_id":"23bfe1eb-0149-437c-9c35-6ab4324afbbf","normalized_state":"finished","queued_time":"2024-12-18T19:22:34.6855798Z","session_start_time":null,"execution_start_time":"2024-12-18T19:22:43.9881329Z","execution_finish_time":"2024-12-18T19:22:47.477348Z","parent_msg_id":"43cd38f7-b844-4798-aec6-64d67d4ac1e6"},"text/plain":"StatementMeta(, 23bfe1eb-0149-437c-9c35-6ab4324afbbf, 77, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["dim_operation upsert completed successfully.\n"]}],"execution_count":69,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"34efb9b8-1a88-43bf-8cfe-b0c20debcf83"},{"cell_type":"code","source":["# Stage and load billing type\n","df_billing_type = spark.sql(\"\"\"\n","    SELECT DISTINCT\n","           abs(hash(billing_type)) billing_type_id,\n","           billing_type\n","    FROM vw_stage_storagebyworkspacesandhour\n","\"\"\")\n","\n","# Define configuration for the upsert operation\n","config = {\n","    'table_name': 'dim_billing_type',\n","    'lakehouse_table_path': lakehouse_table_path,\n","    'source_dataframe': df_billing_type,\n","    'candidate_columns': ['billing_type_id']\n","}\n","\n","# Execute upsert to the dimensional table\n","upsert_to_table(config)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":78,"statement_ids":[78],"state":"finished","livy_statement_state":"available","session_id":"23bfe1eb-0149-437c-9c35-6ab4324afbbf","normalized_state":"finished","queued_time":"2024-12-18T19:22:34.7841966Z","session_start_time":null,"execution_start_time":"2024-12-18T19:22:47.8222481Z","execution_finish_time":"2024-12-18T19:22:50.2712864Z","parent_msg_id":"db55359f-634f-4079-bf4b-72071c32527d"},"text/plain":"StatementMeta(, 23bfe1eb-0149-437c-9c35-6ab4324afbbf, 78, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["dim_billing_type upsert completed successfully.\n"]}],"execution_count":70,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4383c992-dd4e-4b54-b40c-036ff1c08f90"},{"cell_type":"code","source":["# Read capacity\n","spark.read.format(\"delta\").load(f'{lakehouse_table_path}/dim_capacity') \\\n","    .createOrReplaceTempView(\"vw_dim_capacity\")\n","\n","# Stage and load workspace storage\n","df_storage = spark.sql(\"\"\"\n","    SELECT UPPER(workspaceid)        workspace_id,\n","           UPPER(premiumcapacityid)  capacity_id,\n","           CAST(date AS DATE)        date,\n","           datetime                  date_time,\n","           abs(hash(billing_type))   billing_type_id,\n","           abs(hash(operationname))  operation_id,\n","           workloadkind              workload_kind,\n","           storagetype               storage_type,\n","           staticstorageingb         static_storage_gb,\n","           CASE WHEN storagetype = 'OneLake Standard Storage'\n","                THEN staticstorageingb * c.onelake_storage_cost\n","           END                       static_storage_standard_cost,\n","           utilization_gb            utilization_gb\n","    FROM vw_stage_storagebyworkspacesandhour s\n","    LEFT JOIN vw_dim_capacity c\n","           ON UPPER(s.premiumcapacityid) = c.capacity_id\n","\"\"\")\n","\n","# Define configuration for the upsert operation\n","config = {\n","    'table_name': 'fact_workspace_storage',\n","    'lakehouse_table_path': lakehouse_table_path,\n","    'source_dataframe': df_storage,\n","    'candidate_columns': ['workspace_id', 'date_time', 'capacity_id', 'operation_id', 'billing_type_id', 'storage_type']\n","}\n","\n","# Execute upsert to the dimensional table\n","upsert_to_table(config)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":84,"statement_ids":[84],"state":"finished","livy_statement_state":"available","session_id":"23bfe1eb-0149-437c-9c35-6ab4324afbbf","normalized_state":"finished","queued_time":"2024-12-18T19:29:19.2609712Z","session_start_time":null,"execution_start_time":"2024-12-18T19:29:19.5839883Z","execution_finish_time":"2024-12-18T19:29:21.1454389Z","parent_msg_id":"78f52eb6-e42e-43e3-8c1f-98ae6bd204e3"},"text/plain":"StatementMeta(, 23bfe1eb-0149-437c-9c35-6ab4324afbbf, 84, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Table fact_workspace_storage does not exist. Creating it.\nfact_workspace_storage successfully created.\n"]}],"execution_count":76,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b9cf4415-2211-4857-a35c-1111c619d2ac"},{"cell_type":"code","source":["# Stage and load capacity metrics\n","df_metrics = spark.sql(\"\"\"\n","    SELECT UPPER(premiumcapacityid)         capacity_id,\n","           UPPER(m.workspaceid)             workspace_id,\n","           UPPER(m.itemid)                  item_id,\n","           CAST(date AS DATE)               date,\n","           datetime                         date_time,\n","           abs(hash(operationname))         operation_id,\n","           sum_cu                           capacity_unit_seconds_consumed,\n","           sum_cu / 3600                    capacity_units_consumed,\n","           (sum_cu / 3600) * (c.pay_go_hour / c.capacity_unit)          activity_cost_pay_go,\n","           (sum_cu / 3600) * (c.reservation_hour / c.capacity_unit)     activity_cost_reservation,\n","           sum_duration                     activity_duration,\n","           count_operations                 total_operations,\n","           count_users                      users,\n","           avg_durationms                   average_activity_duration,\n","           throttling_min                   minutes_throttled,\n","           count_failure_operations         failed_operations,\n","           count_rejected_operations        rejected_operations,\n","           count_successful_operations      successful_operations,\n","           count_inprogress_operations      in_progress_operations,\n","           count_cancelled_operations       cancelled_operations,\n","           count_invalid_operations         invalid_operations\n","    FROM vw_stage_metricsbyitemandoperationandhour m\n","    LEFT JOIN vw_dim_capacity c\n","           ON UPPER(m.premiumcapacityid) = c.capacity_id\n","    WHERE sum_cu != 0\n","\"\"\")\n","\n","# Define configuration for the upsert operation\n","config = {\n","    'table_name': 'fact_capacity_metrics',\n","    'lakehouse_table_path': lakehouse_table_path,\n","    'source_dataframe': df_metrics,\n","    'candidate_columns': ['date_time', 'operation_id', 'item_id', 'workspace_id', 'capacity_id']\n","}\n","\n","# Execute upsert to the dimensional table\n","upsert_to_table(config)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":83,"statement_ids":[83],"state":"finished","livy_statement_state":"available","session_id":"23bfe1eb-0149-437c-9c35-6ab4324afbbf","normalized_state":"finished","queued_time":"2024-12-18T19:29:08.1360221Z","session_start_time":null,"execution_start_time":"2024-12-18T19:29:08.5902875Z","execution_finish_time":"2024-12-18T19:29:10.9737127Z","parent_msg_id":"c1d16775-4329-4060-ba8f-feda7376c57b"},"text/plain":"StatementMeta(, 23bfe1eb-0149-437c-9c35-6ab4324afbbf, 83, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Table fact_capacity_metrics does not exist. Creating it.\nfact_capacity_metrics successfully created.\n"]}],"execution_count":75,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"advisor":{"adviceMetadata":"{\"artifactId\":\"29ceb676-0647-4bdf-a4b3-3bfdf00fe87f\",\"activityId\":\"23bfe1eb-0149-437c-9c35-6ab4324afbbf\",\"applicationId\":\"application_1734547888069_0001\",\"jobGroupId\":\"83\",\"advices\":{\"info\":1}}"}},"id":"7ed3b123-7c37-4bbe-957a-8e8eff196340"},{"cell_type":"code","source":["# Monitoring semantic model\n","semantic_model = 'Capacity Monitoring'\n","\n","try:\n","    # Retrieve the list of datasets\n","    df_datasets = fabric.list_datasets()\n","    dataset_names = df_datasets['Dataset Name'].tolist()\n","\n","    # Check if the semantic model exists\n","    if semantic_model in dataset_names:\n","        print(f'Semantic model {semantic_model} already exists. Skipping creation.')\n","    else:\n","        import requests\n","        import json\n","\n","        # Define .bim file URL\n","        bim_url = 'https://raw.githubusercontent.com/Lucid-Will/Lucid-Capacity-Monitoring/main/semantic_model_bim/lucid_capacity_monitor.bim'\n","\n","        # Download and load BIM file as JSON\n","        response = requests.get(bim_url)\n","        bim_json = response.json()\n","\n","        # Deploy new semantic model using the BIM JSON\n","        sl.create_semantic_model_from_bim(semantic_model, bim_json)\n","        print(f'Semantic model {semantic_model} created.')\n","\n","        # Attach semantic model to monitoring lakehouse\n","        sl.directlake.update_direct_lake_model_lakehouse_connection(dataset=semantic_model, workspace=None, lakehouse=monitoring_lakehouse) \n","\n","except Exception as e:\n","    print(f'An unexpected error occurred while processing the semantic model: {e}')"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":81,"statement_ids":[81],"state":"finished","livy_statement_state":"available","session_id":"23bfe1eb-0149-437c-9c35-6ab4324afbbf","normalized_state":"finished","queued_time":"2024-12-18T19:22:35.1065604Z","session_start_time":null,"execution_start_time":"2024-12-18T19:23:00.9959699Z","execution_finish_time":"2024-12-18T19:23:01.2322029Z","parent_msg_id":"b7408f00-9d3e-44cb-951d-e6786cae0bb8"},"text/plain":"StatementMeta(, 23bfe1eb-0149-437c-9c35-6ab4324afbbf, 81, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Semantic model Capacity Monitoring already exists. Skipping creation.\n"]}],"execution_count":73,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false,"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4dabc2b1-6cfc-4a49-a019-8825903565c7"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","ms_spell_check":{"ms_spell_check_language":"en"},"language_group":"synapse_pyspark"},"nteract":{"version":"nteract-front-end@1.0.0"},"widgets":{},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"default_lakehouse":"5a51082e-0ffe-4894-9335-24bba26ec6d2","default_lakehouse_name":"lh_control","default_lakehouse_workspace_id":"99acdea9-fc11-42df-aebc-dd873e1db1f5"}}},"nbformat":4,"nbformat_minor":5}